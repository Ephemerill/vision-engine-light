<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <title>Pro Body & Face Detection — Improved</title>

    <!-- Tailwind -->
    <script src="https://cdn.tailwindcss.com"></script>

    <!-- Google Fonts -->
    <link
        href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;700&family=Inter:wght@400;600;700&display=swap"
        rel="stylesheet">

    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #020617;
            color: white;
            overflow: hidden;
        }

        .video-container {
            position: relative;
            width: 100%;
            max-width: 1280px;
            aspect-ratio: 16/9;
            margin: 0 auto;
            border-radius: 12px;
            overflow: hidden;
            background: #000;
            box-shadow: 0 0 40px rgba(56, 189, 248, 0.07);
            border: 1px solid rgba(255, 255, 255, 0.06);
        }

        video,
        canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            object-fit: cover;
            transform: scaleX(-1);
        }

        canvas {
            z-index: 10;
        }

        .mono {
            font-family: 'JetBrains Mono', monospace;
        }

        .loader {
            border: 3px solid rgba(255, 255, 255, 0.08);
            border-left-color: #f43f5e;
            border-radius: 50%;
            width: 30px;
            height: 30px;
            animation: spin 1s linear infinite;
        }

        @keyframes spin {
            from {
                transform: rotate(0);
            }

            to {
                transform: rotate(360deg);
            }
        }

        .hud-panel {
            background: rgba(15, 23, 42, 0.8);
            backdrop-filter: blur(8px);
            border: 1px solid rgba(255, 255, 255, 0.06);
            border-radius: 8px;
        }
    </style>
</head>

<body class="flex flex-col items-center justify-center min-h-screen p-4">

    <header class="w-full max-w-6xl flex justify-between items-center mb-6">
        <div>
            <h1 class="text-2xl font-bold text-white tracking-tight">Neural<span class="text-rose-500">Track</span> Pro
            </h1>
            <p class="text-slate-400 text-xs uppercase tracking-widest mt-1">High-Fidelity Pipeline</p>
        </div>
        <div id="status-badge"
            class="px-3 py-1 rounded-full text-xs font-bold mono bg-slate-800 text-slate-400 border border-slate-700 flex items-center gap-2">
            <div class="w-2 h-2 rounded-full bg-slate-500 animate-pulse"></div>INITIALIZING
        </div>
    </header>

    <main class="w-full flex flex-col items-center">
        <div class="video-container group">
            <video id="webcam" autoplay playsinline muted></video>
            <canvas id="output_canvas"></canvas>

            <div id="start-overlay"
                class="absolute inset-0 flex flex-col items-center justify-center bg-slate-950/90 z-30 transition-opacity duration-500">
                <div id="loading-spinner" class="loader mb-4"></div>
                <p id="loading-text" class="text-slate-300 mb-6 font-medium mono text-sm">LOADING MODELS...</p>

                <button id="enableWebcamButton"
                    class="hidden group relative px-8 py-3 bg-rose-600 hover:bg-rose-500 rounded-lg font-bold text-white shadow-lg transition-all hover:scale-105 active:scale-95">
                    <span class="relative z-10 flex items-center gap-2">INITIATE SENSORS</span>
                </button>
                <p id="error-msg" class="hidden text-red-400 mt-4 text-sm mono text-center"></p>
            </div>

            <div class="absolute top-4 left-4 z-20 flex flex-col gap-2">
                <div class="hud-panel px-3 py-2">
                    <div class="text-[10px] text-slate-400 uppercase tracking-wider mb-1">Telemetry</div>
                    <div class="mono text-rose-400 text-sm">FPS: <span id="fps-counter" class="text-white">0</span>
                    </div>
                </div>
            </div>

            <div class="absolute bottom-4 left-4 z-20 flex gap-4">
                <div
                    class="flex items-center gap-2 px-3 py-1.5 rounded-md bg-black/60 backdrop-blur border border-white/10">
                    <div class="w-3 h-3 bg-indigo-500 rounded-sm"></div>
                    <span class="text-xs text-slate-200 font-medium">Person (Lite2)</span>
                </div>
                <div
                    class="flex items-center gap-2 px-3 py-1.5 rounded-md bg-black/60 backdrop-blur border border-white/10">
                    <div class="w-3 h-3 bg-rose-500 rounded-sm"></div>
                    <span class="text-xs text-slate-200 font-medium">Face (Mesh + Tracking)</span>
                </div>
            </div>
        </div>
    </main>

    <div class="mt-6 flex justify-center w-full max-w-lg">
        <p class="text-xs text-slate-500 text-center leading-relaxed">
            Running <strong>EfficientDet-Lite2</strong> for bodies and <strong>FaceLandmarker</strong> for faces with
            temporal smoothing. Works best with GPU delegate.
        </p>
    </div>

    <script type="module">
        import { ObjectDetector, FaceLandmarker, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";

        // UI elements
        const VIDEO = document.getElementById("webcam");
        const CANVAS = document.getElementById("output_canvas");
        const CTX = CANVAS.getContext("2d");
        const START_OVERLAY = document.getElementById("start-overlay");
        const ENABLE_BTN = document.getElementById("enableWebcamButton");
        const LOADING_SPINNER = document.getElementById("loading-spinner");
        const LOADING_TEXT = document.getElementById("loading-text");
        const STATUS_BADGE = document.getElementById("status-badge");
        const ERROR_MSG = document.getElementById("error-msg");
        const FPS_COUNTER = document.getElementById("fps-counter");

        let objectDetector = null;
        let faceLandmarker = null;

        // Tracking state (simple temporal smoothing + ID assignment)
        let trackedFaces = []; // { id, smoothedBox, lastSeenTimestamp }
        let nextFaceId = 1;
        const SMOOTH_ALPHA = 0.25;      // smoothing factor for exponential smoothing
        const MAX_MISSED_MS = 350;     // how long to keep a face alive without re-detection
        const MATCH_IOU_THRESHOLD = 0.35;

        // FPS
        let lastFrameTime = performance.now();
        let frameCount = 0;
        let fps = 0;
        let lastFpsUpdate = performance.now();

        async function initializeModels() {
            try {
                const vision = await FilesetResolver.forVisionTasks(
                    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
                );

                // Face Landmarker (serves detection + landmarks). We'll tune thresholds to increase pickup.
                faceLandmarker = await FaceLandmarker.createFromOptions(vision, {
                    baseOptions: {
                        modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
                        delegate: "GPU"
                    },
                    runningMode: "VIDEO",
                    numFaces: 5,
                    outputFaceBlendshapes: false,
                    minFaceDetectionConfidence: 0.40,    // slightly lower to pick up small/partial faces
                    minFacePresenceConfidence: 0.35,
                    minTrackingConfidence: 0.30
                });

                // Object detector for person/body boxes
                objectDetector = await ObjectDetector.createFromOptions(vision, {
                    baseOptions: {
                        modelAssetPath: `https://storage.googleapis.com/mediapipe-models/object_detector/efficientdet_lite2/float16/1/efficientdet_lite2.tflite`,
                        delegate: "GPU"
                    },
                    scoreThreshold: 0.45,
                    runningMode: "VIDEO",
                    categoryAllowlist: ["person"]
                });

                // UI update
                LOADING_SPINNER.classList.add("hidden");
                LOADING_TEXT.classList.add("hidden");
                ENABLE_BTN.classList.remove("hidden");
                STATUS_BADGE.className = "px-3 py-1 rounded-full text-xs font-bold mono bg-emerald-900/30 text-emerald-400 border border-emerald-500/30 flex items-center gap-2";
                STATUS_BADGE.innerHTML = `<div class="w-2 h-2 rounded-full bg-emerald-400 shadow-[0_0_10px_#34d399]"></div>SYSTEM ONLINE`;

            } catch (e) {
                console.error("Model init error:", e);
                LOADING_SPINNER.classList.add("hidden");
                LOADING_TEXT.classList.add("hidden");
                ERROR_MSG.textContent = "Failed to load models or GPU delegate unavailable.";
                ERROR_MSG.classList.remove("hidden");
            }
        }

        initializeModels();

        // Start camera
        ENABLE_BTN.addEventListener("click", async () => {
            if (!faceLandmarker || !objectDetector) return;
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        width: { ideal: 1280 },
                        height: { ideal: 720 },
                        facingMode: "user"
                    },
                    audio: false
                });
                VIDEO.srcObject = stream;
                VIDEO.addEventListener("loadeddata", () => {
                    // hide overlay and start loop
                    START_OVERLAY.classList.add("opacity-0", "pointer-events-none");
                    requestAnimationFrame(processFrame);
                }, { once: true });
            } catch (err) {
                console.error(err);
                ERROR_MSG.textContent = "Camera access denied.";
                ERROR_MSG.classList.remove("hidden");
            }
        });

        // Helper — compute bounding box from landmarks array (normalized coords)
        function getBoxFromLandmarks(landmarks, width, height, pad = 0.12) {
            let xMin = 1, yMin = 1, xMax = 0, yMax = 0;
            for (const p of landmarks) {
                if (p.x < xMin) xMin = p.x;
                if (p.x > xMax) xMax = p.x;
                if (p.y < yMin) yMin = p.y;
                if (p.y > yMax) yMax = p.y;
            }
            // pad as fraction of box
            const w = xMax - xMin;
            const h = yMax - yMin;
            const cx = (xMin + xMax) / 2;
            const cy = (yMin + yMax) / 2;
            const pw = w * (1 + pad * 2);
            const ph = h * (1 + pad * 2);
            const nx1 = cx - pw / 2;
            const ny1 = cy - ph / 2;
            return {
                originX: clamp(nx1 * width, 0, width),
                originY: clamp(ny1 * height, 0, height),
                width: clamp(pw * width, 0, width),
                height: clamp(ph * height, 0, height)
            };
        }

        function clamp(v, a, b) { return Math.max(a, Math.min(b, v)); }

        // IoU for matching boxes (box: { originX, originY, width, height })
        function iou(a, b) {
            const ax1 = a.originX, ay1 = a.originY, ax2 = a.originX + a.width, ay2 = a.originY + a.height;
            const bx1 = b.originX, by1 = b.originY, bx2 = b.originX + b.width, by2 = b.originY + b.height;
            const interX1 = Math.max(ax1, bx1), interY1 = Math.max(ay1, by1);
            const interX2 = Math.min(ax2, bx2), interY2 = Math.min(ay2, by2);
            const interW = Math.max(0, interX2 - interX1);
            const interH = Math.max(0, interY2 - interY1);
            const interArea = interW * interH;
            const unionArea = (a.width * a.height) + (b.width * b.height) - interArea;
            return unionArea === 0 ? 0 : interArea / unionArea;
        }

        // Smooth box using exponential moving average
        function smoothBox(oldBox, newBox, alpha) {
            if (!oldBox) return { ...newBox };
            return {
                originX: oldBox.originX * (1 - alpha) + newBox.originX * alpha,
                originY: oldBox.originY * (1 - alpha) + newBox.originY * alpha,
                width: oldBox.width * (1 - alpha) + newBox.width * alpha,
                height: oldBox.height * (1 - alpha) + newBox.height * alpha
            };
        }

        // Main loop
        async function processFrame() {
            if (!VIDEO || VIDEO.readyState < 2) {
                requestAnimationFrame(processFrame);
                return;
            }

            // resize canvas to match video
            if (CANVAS.width !== VIDEO.videoWidth || CANVAS.height !== VIDEO.videoHeight) {
                CANVAS.width = VIDEO.videoWidth;
                CANVAS.height = VIDEO.videoHeight;
            }

            const now = performance.now();

            // Dispatch both detectors. faceLandmarker is used for faces (detection + landmarks)
            // objectDetector used for person boxes
            let objectPromise = null;
            let facePromise = null;

            if (objectDetector) {
                try {
                    objectPromise = objectDetector.detectForVideo(VIDEO, now);
                } catch (e) {
                    console.warn("object detect error:", e);
                }
            }
            if (faceLandmarker) {
                try {
                    // We call detectForVideo every frame to maximize face pickup.
                    facePromise = faceLandmarker.detectForVideo(VIDEO, now);
                } catch (e) {
                    console.warn("face detect error:", e);
                }
            }

            // Await results (if promises exist). If either is null, treat as empty.
            let objectResult = null;
            let faceResult = null;
            try {
                if (objectPromise) objectResult = await objectPromise;
            } catch (e) {
                console.warn("object await error:", e);
            }
            try {
                if (facePromise) faceResult = await facePromise;
            } catch (e) {
                console.warn("face await error:", e);
            }

            // Clear canvas
            CTX.clearRect(0, 0, CANVAS.width, CANVAS.height);

            // Draw persons
            if (objectResult && objectResult.detections) {
                for (const detection of objectResult.detections) {
                    if (!detection.boundingBox) continue;
                    drawFancyBox(detection.boundingBox, "#6366f1", "PERSON", detection.categories?.[0]?.score ?? 0.0, 0.04);
                }
            }

            // Build face boxes from landmarks if present
            const detectedFaceBoxes = [];
            if (faceResult && faceResult.faceLandmarks) {
                for (const landmarks of faceResult.faceLandmarks) {
                    const box = getBoxFromLandmarks(landmarks, CANVAS.width, CANVAS.height, 0.20); // extra padding
                    detectedFaceBoxes.push({ box, landmarks });
                }
            }

            // ----- TRACKING & SMOOTHING -----
            // Match detectedFaceBoxes to trackedFaces by IoU, update smoothed boxes
            const timestamp = performance.now();

            // mark all tracked as unmatched initially
            for (const t of trackedFaces) t._matched = false;

            // greedy matching: for each detected box find best track above threshold
            for (const detected of detectedFaceBoxes) {
                let bestMatch = null;
                let bestIoU = 0;
                for (const t of trackedFaces) {
                    const score = iou(t.smoothedBox, detected.box);
                    if (score > bestIoU) { bestIoU = score; bestMatch = t; }
                }
                if (bestMatch && bestIoU >= MATCH_IOU_THRESHOLD) {
                    // update matched track
                    bestMatch.smoothedBox = smoothBox(bestMatch.smoothedBox, detected.box, SMOOTH_ALPHA);
                    bestMatch.lastSeen = timestamp;
                    bestMatch._matched = true;
                    bestMatch._lastLandmarks = detected.landmarks;
                } else {
                    // no match -> create new track
                    const nf = {
                        id: nextFaceId++,
                        smoothedBox: { ...detected.box },
                        lastSeen: timestamp,
                        _matched: true,
                        _lastLandmarks: detected.landmarks
                    };
                    trackedFaces.push(nf);
                }
            }

            // Remove stale tracks
            trackedFaces = trackedFaces.filter(t => (timestamp - t.lastSeen) <= MAX_MISSED_MS);

            // Draw tracked faces (smoothed boxes). Use last seen confidence when available.
            for (const t of trackedFaces) {
                drawFancyBox(t.smoothedBox, "#f43f5e", `FACE #${t.id}`, 0.98, 0.04);
                // Optionally draw landmarks
                if (t._lastLandmarks) {
                    drawLandmarks(t._lastLandmarks, CANVAS.width, CANVAS.height);
                }
            }

            // FPS counting
            frameCount++;
            const cur = performance.now();
            if (cur - lastFpsUpdate >= 1000) {
                fps = frameCount;
                frameCount = 0;
                lastFpsUpdate = cur;
                FPS_COUNTER.innerText = fps;
            }

            requestAnimationFrame(processFrame);
        }

        // Drawing helpers
        function drawLandmarks(landmarks, width, height) {
            CTX.save();
            CTX.fillStyle = "rgba(244,63,94,0.95)";
            CTX.strokeStyle = "rgba(0,0,0,0.6)";
            CTX.lineWidth = 1;
            for (let i = 0; i < landmarks.length; i++) {
                const p = landmarks[i];
                const x = p.x * width;
                const y = p.y * height;
                CTX.beginPath();
                CTX.arc(x, y, 1.6, 0, Math.PI * 2);
                CTX.fill();
                CTX.stroke();
            }
            CTX.restore();
        }

        // Fancy box drawing (matches your style). Accepts either normalized {xCenter,yCenter,w,h} OR concrete origin/width/height.
        function drawFancyBox(box, color, label, score = 0.0, fillOpacity = 0.06) {
            // Accept Mediapipe boundingBox shape or our origin/width/height
            let originX, originY, width, height;
            if (box.originX !== undefined) {
                originX = box.originX; originY = box.originY; width = box.width; height = box.height;
            } else if (box.xCenter !== undefined && box.width !== undefined) {
                // fallback for some object detectors that return normalized centers
                const cx = box.xCenter * CANVAS.width;
                const cy = box.yCenter * CANVAS.height;
                width = box.width * CANVAS.width;
                height = box.height * CANVAS.height;
                originX = cx - width / 2;
                originY = cy - height / 2;
            } else {
                // unknown format
                return;
            }

            CTX.save();

            const lineLen = Math.min(width, height) * 0.18;
            CTX.strokeStyle = color;
            CTX.lineWidth = 3;
            CTX.lineCap = "round";

            CTX.beginPath();
            // TL
            CTX.moveTo(originX, originY + lineLen);
            CTX.lineTo(originX, originY);
            CTX.lineTo(originX + lineLen, originY);
            // TR
            CTX.moveTo(originX + width - lineLen, originY);
            CTX.lineTo(originX + width, originY);
            CTX.lineTo(originX + width, originY + lineLen);
            // BR
            CTX.moveTo(originX + width, originY + height - lineLen);
            CTX.lineTo(originX + width, originY + height);
            CTX.lineTo(originX + width - lineLen, originY + height);
            // BL
            CTX.moveTo(originX + lineLen, originY + height);
            CTX.lineTo(originX, originY + height);
            CTX.lineTo(originX, originY + height - lineLen);
            CTX.stroke();

            // Slight fill
            // convert hex color to rgba with low alpha using canvas trick (fillStyle accepts rgba)
            CTX.fillStyle = hexToRgba(color, fillOpacity);
            CTX.fillRect(originX, originY, width, height);

            // Label background
            const text = `${label} ${(score * 100).toFixed(0)}%`;
            CTX.font = "bold 12px 'JetBrains Mono', monospace";
            const tm = CTX.measureText(text);
            const txtW = tm.width + 12;
            const txtH = 22;
            const labelX = Math.max(4, originX);
            const labelY = Math.max(originY - txtH - 4, 4);

            CTX.fillStyle = color;
            CTX.fillRect(labelX, labelY, txtW, txtH);

            CTX.fillStyle = "#000";
            CTX.fillText(text, labelX + 6, labelY + 15);

            CTX.restore();
        }

        // hex color to rgba; accepts "#rrggbb" or "rgb(...)" too
        function hexToRgba(hex, alpha) {
            if (hex.startsWith("#")) {
                const r = parseInt(hex.slice(1, 3), 16);
                const g = parseInt(hex.slice(3, 5), 16);
                const b = parseInt(hex.slice(5, 7), 16);
                return `rgba(${r},${g},${b},${alpha})`;
            }
            // fallback
            return hex;
        }

    </script>
</body>

</html>